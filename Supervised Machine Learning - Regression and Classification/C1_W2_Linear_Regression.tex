\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{C1\_W2\_Linear\_Regression}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{practice-lab-linear-regression}{%
\section{Practice Lab: Linear
Regression}\label{practice-lab-linear-regression}}

Welcome to your first practice lab! In this lab, you will implement
linear regression with one variable to predict profits for a restaurant
franchise.

\hypertarget{outline}{%
\section{Outline}\label{outline}}

\begin{itemize}
\tightlist
\item
  Section \ref{1}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{21}
  \item
    Section \ref{22}
  \item
    Section \ref{23}
  \item
    Section \ref{24}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex01}
    \end{itemize}
  \item
    Section \ref{25}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex02}
    \end{itemize}
  \item
    Section \ref{26}
  \end{itemize}
\end{itemize}

    \emph{\textbf{NOTE:} To prevent errors from the autograder, you are not
allowed to edit or delete non-graded cells in this notebook . Please
also refrain from adding any new cells. \textbf{Once you have passed
this assignment} and want to experiment with any of the non-graded code,
you may follow the instructions at the bottom of this notebook.}

    \#\# 1 - Packages

First, let's run the cell below to import all the packages that you will
need during this assignment. - \href{www.numpy.org}{numpy} is the
fundamental package for working with matrices in Python. -
\href{http://matplotlib.org}{matplotlib} is a famous library to plot
graphs in Python. - \texttt{utils.py} contains helper functions for this
assignment. You do not need to modify code in this file.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{utils} \PY{k+kn}{import} \PY{o}{*}
\PY{k+kn}{import} \PY{n+nn}{copy}
\PY{k+kn}{import} \PY{n+nn}{math}
\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{problem-statement}{%
\subsection{2 - Problem Statement}\label{problem-statement}}

Suppose you are the CEO of a restaurant franchise and are considering
different cities for opening a new outlet. - You would like to expand
your business to cities that may give your restaurant higher profits. -
The chain already has restaurants in various cities and you have data
for profits and populations from the cities. - You also have data on
cities that are candidates for a new restaurant. - For these cities, you
have the city population.

Can you use the data to help you identify which cities may potentially
give your business higher profits?

\hypertarget{dataset}{%
\subsection{3 - Dataset}\label{dataset}}

You will start by loading the dataset for this task. - The
\texttt{load\_data()} function shown below loads the data into variables
\texttt{x\_train} and \texttt{y\_train} - \texttt{x\_train} is the
population of a city - \texttt{y\_train} is the profit of a restaurant
in that city. A negative value for profit indicates a loss.\\
- Both \texttt{X\_train} and \texttt{y\_train} are numpy arrays.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} load the dataset}
\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{view-the-variables}{%
\paragraph{View the variables}\label{view-the-variables}}

Before starting on any task, it is useful to get more familiar with your
dataset.\\
- A good place to start is to just print out each variable and see what
it contains.

The code below prints the variable \texttt{x\_train} and the type of the
variable.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print x\PYZus{}train}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Type of x\PYZus{}train:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First five elements of x\PYZus{}train are:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Type of x\_train: <class 'numpy.ndarray'>
First five elements of x\_train are:
 [6.1101 5.5277 8.5186 7.0032 5.8598]
    \end{Verbatim}

    \texttt{x\_train} is a numpy array that contains decimal values that are
all greater than zero. - These values represent the city population
times 10,000 - For example, 6.1101 means that the population for that
city is 61,101

Now, let's print \texttt{y\_train}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print y\PYZus{}train}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Type of y\PYZus{}train:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First five elements of y\PYZus{}train are:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}  
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Type of y\_train: <class 'numpy.ndarray'>
First five elements of y\_train are:
 [17.592   9.1302 13.662  11.854   6.8233]
    \end{Verbatim}

    Similarly, \texttt{y\_train} is a numpy array that has decimal values,
some negative, some positive. - These represent your restaurant's
average monthly profits in each city, in units of \$10,000. - For
example, 17.592 represents \$175,920 in average monthly profits for that
city. - -2.6807 represents -\$26,807 in average monthly loss for that
city.

    \hypertarget{check-the-dimensions-of-your-variables}{%
\paragraph{Check the dimensions of your
variables}\label{check-the-dimensions-of-your-variables}}

Another useful way to get familiar with your data is to view its
dimensions.

Please print the shape of \texttt{x\_train} and \texttt{y\_train} and
see how many training examples you have in your dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The shape of x\PYZus{}train is:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The shape of y\PYZus{}train is: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of training examples (m):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
The shape of x\_train is: (97,)
The shape of y\_train is:  (97,)
Number of training examples (m): 97
    \end{Verbatim}

    The city population array has 97 data points, and the monthly average
profits also has 97 data points. These are NumPy 1D arrays.

    \hypertarget{visualize-your-data}{%
\paragraph{Visualize your data}\label{visualize-your-data}}

It is often useful to understand the data by visualizing it. - For this
dataset, you can use a scatter plot to visualize the data, since it has
only two properties to plot (profit and population). - Many other
problems that you will encounter in real life have more than two
properties (for example, population, average household income, monthly
profits, monthly sales).When you have more than two properties, you can
still use a scatter plot to see the relationship between each pair of
properties.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create a scatter plot of the data. To change the markers to red \PYZdq{}x\PYZdq{},}
\PY{c+c1}{\PYZsh{} we used the \PYZsq{}marker\PYZsq{} and \PYZsq{}c\PYZsq{} parameters}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 

\PY{c+c1}{\PYZsh{} Set the title}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Profits vs. Population per city}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Set the y\PYZhy{}axis label}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Profit in \PYZdl{}10,000}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Set the x\PYZhy{}axis label}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Population of City in 10,000s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Your goal is to build a linear regression model to fit this data. - With
this model, you can then input a new city's population, and have the
model estimate your restaurant's potential monthly profits for that
city.

    \#\# 4 - Refresher on linear regression

In this practice lab, you will fit the linear regression parameters
\((w,b)\) to your dataset. - The model function for linear regression,
which is a function that maps from \texttt{x} (city population) to
\texttt{y} (your restaurant's monthly profit for that city) is
represented as \[f_{w,b}(x) = wx + b\]

\begin{itemize}
\item
  To train a linear regression model, you want to find the best
  \((w,b)\) parameters that fit your dataset.

  \begin{itemize}
  \tightlist
  \item
    To compare how one choice of \((w,b)\) is better or worse than
    another choice, you can evaluate it with a cost function \(J(w,b)\)

    \begin{itemize}
    \tightlist
    \item
      \(J\) is a function of \((w,b)\). That is, the value of the cost
      \(J(w,b)\) depends on the value of \((w,b)\).
    \end{itemize}
  \item
    The choice of \((w,b)\) that fits your data the best is the one that
    has the smallest cost \(J(w,b)\).
  \end{itemize}
\item
  To find the values \((w,b)\) that gets the smallest possible cost
  \(J(w,b)\), you can use a method called \textbf{gradient descent}.

  \begin{itemize}
  \tightlist
  \item
    With each step of gradient descent, your parameters \((w,b)\) come
    closer to the optimal values that will achieve the lowest cost
    \(J(w,b)\).
  \end{itemize}
\item
  The trained linear regression model can then take the input feature
  \(x\) (city population) and output a prediction \(f_{w,b}(x)\)
  (predicted monthly profit for a restaurant in that city).
\end{itemize}

    \#\# 5 - Compute Cost

Gradient descent involves repeated steps to adjust the value of your
parameter \((w,b)\) to gradually get a smaller and smaller cost
\(J(w,b)\). - At each step of gradient descent, it will be helpful for
you to monitor your progress by computing the cost \(J(w,b)\) as
\((w,b)\) gets updated. - In this section, you will implement a function
to calculate \(J(w,b)\) so that you can check the progress of your
gradient descent implementation.

\hypertarget{cost-function}{%
\paragraph{Cost function}\label{cost-function}}

As you may recall from the lecture, for one variable, the cost function
for linear regression \(J(w,b)\) is defined as

\[J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\]

\begin{itemize}
\tightlist
\item
  You can think of \(f_{w,b}(x^{(i)})\) as the model's prediction of
  your restaurant's profit, as opposed to \(y^{(i)}\), which is the
  actual profit that is recorded in the data.
\item
  \(m\) is the number of training examples in the dataset
\end{itemize}

\hypertarget{model-prediction}{%
\paragraph{Model prediction}\label{model-prediction}}

\begin{itemize}
\tightlist
\item
  For linear regression with one variable, the prediction of the model
  \(f_{w,b}\) for an example \(x^{(i)}\) is representented as:
\end{itemize}

\[ f_{w,b}(x^{(i)}) = wx^{(i)} + b\]

This is the equation for a line, with an intercept \(b\) and a slope
\(w\)

\hypertarget{implementation}{%
\paragraph{Implementation}\label{implementation}}

Please complete the \texttt{compute\_cost()} function below to compute
the cost \(J(w,b)\).

    \#\#\# Exercise 1

Complete the \texttt{compute\_cost} below to:

\begin{itemize}
\tightlist
\item
  Iterate over the training examples, and for each example, compute:

  \begin{itemize}
  \item
    The prediction of the model for that example \[
      f_{wb}(x^{(i)}) =  wx^{(i)} + b 
      \]
  \item
    The cost for that example \[cost^{(i)} =  (f_{wb} - y^{(i)})^2\]
  \end{itemize}
\item
  Return the total cost over all examples
  \[J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} cost^{(i)}\]

  \begin{itemize}
  \tightlist
  \item
    Here, \(m\) is the number of training examples and \(\sum\) is the
    summation operator
  \end{itemize}
\end{itemize}

If you get stuck, you can check out the hints presented after the cell
below to help you with the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}cost}

\PY{k}{def} \PY{n+nf}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:} 
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the cost function for linear regression.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        x (ndarray): Shape (m,) Input to the model (Population of cities) }
\PY{l+s+sd}{        y (ndarray): Shape (m,) Label (Actual profits for the cities)}
\PY{l+s+sd}{        w, b (scalar): Parameters of the model}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{        total\PYZus{}cost (float): The cost of using w,b as the parameters for linear regression}
\PY{l+s+sd}{               to fit the data points in x and y}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} number of training examples}
    \PY{n}{m} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} 
    
    \PY{c+c1}{\PYZsh{} You need to return this variable correctly}
    \PY{n}{total\PYZus{}cost} \PY{o}{=} \PY{l+m+mi}{0}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
        \PY{n}{total\PYZus{}cost} \PY{o}{=} \PY{n}{total\PYZus{}cost} \PY{o}{+} \PY{p}{(}\PY{p}{(}\PY{n}{w} \PY{o}{*} \PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+} \PY{n}{b}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{n}{total\PYZus{}cost} \PY{o}{=} \PY{n}{total\PYZus{}cost}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{m}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} }

    \PY{k}{return} \PY{n}{total\PYZus{}cost}
\end{Verbatim}
\end{tcolorbox}

    Click for hints

\begin{itemize}
\item
  You can represent a summation operator eg:
  \(h = \sum\limits_{i = 0}^{m-1} 2i\) in code as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(m):}
\NormalTok{    h }\OperatorTok{=}\NormalTok{ h }\OperatorTok{+} \DecValTok{2}\OperatorTok{*}\NormalTok{i}
\end{Highlighting}
\end{Shaded}

  \begin{itemize}
  \item
    In this case, you can iterate over all the examples in \texttt{x}
    using a for loop and add the \texttt{cost} from each iteration to a
    variable (\texttt{cost\_sum}) initialized outside the loop.
  \item
    Then, you can return the \texttt{total\_cost} as \texttt{cost\_sum}
    divided by \texttt{2m}.
  \item
    If you are new to Python, please check that your code is properly
    indented with consistent spaces or tabs. Otherwise, it might produce
    a different output or raise an
    \texttt{IndentationError:\ unexpected\ indent} error. You can refer
    to
    \href{https://community.deeplearning.ai/t/indentation-in-python-indentationerror-unexpected-indent/159398}{this
    topic} in our community for details.
  \end{itemize}

  Click for more hints

  \begin{itemize}
  \tightlist
  \item
    Here's how you can structure the overall implementation for this
    function
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{def}\NormalTok{ compute\_cost(x, y, w, b):}
     \CommentTok{\# number of training examples}
\NormalTok{     m }\OperatorTok{=}\NormalTok{ x.shape[}\DecValTok{0}\NormalTok{] }

     \CommentTok{\# You need to return this variable correctly}
\NormalTok{     total\_cost }\OperatorTok{=} \DecValTok{0}

     \CommentTok{\#\#\# START CODE HERE }\AlertTok{\#\#\#}\CommentTok{  }
     \CommentTok{\# Variable to keep track of sum of cost from each example}
\NormalTok{     cost\_sum }\OperatorTok{=} \DecValTok{0}

     \CommentTok{\# Loop over training examples}
     \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(m):}
         \CommentTok{\# Your code here to get the prediction f\_wb for the ith example}
\NormalTok{         f\_wb }\OperatorTok{=} 
         \CommentTok{\# Your code here to get the cost associated with the ith example}
\NormalTok{         cost }\OperatorTok{=} 

         \CommentTok{\# Add to sum of cost for each example}
\NormalTok{         cost\_sum }\OperatorTok{=}\NormalTok{ cost\_sum }\OperatorTok{+}\NormalTok{ cost }

     \CommentTok{\# Get the total cost as the sum divided by (2*m)}
\NormalTok{     total\_cost }\OperatorTok{=}\NormalTok{ (}\DecValTok{1} \OperatorTok{/}\NormalTok{ (}\DecValTok{2} \OperatorTok{*}\NormalTok{ m)) }\OperatorTok{*}\NormalTok{ cost\_sum}
     \CommentTok{\#\#\# }\RegionMarkerTok{END}\CommentTok{ CODE HERE }\AlertTok{\#\#\#}\CommentTok{ }

     \ControlFlowTok{return}\NormalTok{ total\_cost}
\end{Highlighting}
\end{Shaded}

  \begin{itemize}
  \tightlist
  \item
    If you're still stuck, you can check the hints presented below to
    figure out how to calculate \texttt{f\_wb} and \texttt{cost}.
  \end{itemize}

  Hint to calculate f\_wb     For scalars \(a\), \(b\) and \(c\)
  (x{[}i{]}, w and b are all scalars), you can calculate the equation
  \(h = ab + c\) in code as h = a * b + c

      More hints to calculate f     You can compute f\_wb as f\_wb = w *
  x{[}i{]} + b

  Hint to calculate cost     You can calculate the square of a variable
  z as z**2

      More hints to calculate cost     You can compute cost as cost =
  (f\_wb - y{[}i{]}) ** 2
\end{itemize}

    You can check if your implementation was correct by running the
following test code:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Compute cost with some initial values for paramaters w, b}
\PY{n}{initial\PYZus{}w} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{l+m+mi}{1}

\PY{n}{cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{cost}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cost at initial w: }\PY{l+s+si}{\PYZob{}}\PY{n}{cost}\PY{l+s+si}{:}\PY{l+s+s1}{.3f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Public tests}
\PY{k+kn}{from} \PY{n+nn}{public\PYZus{}tests} \PY{k+kn}{import} \PY{o}{*}
\PY{n}{compute\PYZus{}cost\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}cost}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'numpy.float64'>
Cost at initial w: 75.203
\textcolor{ansi-green-intense}{All tests passed!}
    \end{Verbatim}

    \textbf{Expected Output}:

Cost at initial w: 75.203

    \#\# 6 - Gradient descent

In this section, you will implement the gradient for parameters \(w, b\)
for linear regression.

    As described in the lecture videos, the gradient descent algorithm is:

\[\begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \phantom {0000} b := b -  \alpha \frac{\partial J(w,b)}{\partial b} \newline       \; & \phantom {0000} w := w -  \alpha \frac{\partial J(w,b)}{\partial w} \tag{1}  \; & 
\newline & \rbrace\end{align*}\]

where, parameters \(w, b\) are both updated simultaniously and where\\
\[
\frac{\partial J(w,b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \tag{2}
\] \[
\frac{\partial J(w,b)}{\partial w}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \tag{3}
\] * m is the number of training examples in the dataset

\begin{itemize}
\tightlist
\item
  \(f_{w,b}(x^{(i)})\) is the model's prediction, while \(y^{(i)}\), is
  the target value
\end{itemize}

You will implement a function called \texttt{compute\_gradient} which
calculates \(\frac{\partial J(w)}{\partial w}\),
\(\frac{\partial J(w)}{\partial b}\)

    \#\#\# Exercise 2

Please complete the \texttt{compute\_gradient} function to:

\begin{itemize}
\item
  Iterate over the training examples, and for each example, compute:

  \begin{itemize}
  \item
    The prediction of the model for that example \[
      f_{wb}(x^{(i)}) =  wx^{(i)} + b 
      \]
  \item
    The gradient for the parameters \(w, b\) from that example \[
      \frac{\partial J(w,b)}{\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)}) 
      \] \[
      \frac{\partial J(w,b)}{\partial w}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} 
      \]
  \end{itemize}
\item
  Return the total gradient update from all the examples \[
    \frac{\partial J(w,b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} \frac{\partial J(w,b)}{\partial b}^{(i)}
    \]

  \[
    \frac{\partial J(w,b)}{\partial w}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} \frac{\partial J(w,b)}{\partial w}^{(i)} 
    \]

  \begin{itemize}
  \tightlist
  \item
    Here, \(m\) is the number of training examples and \(\sum\) is the
    summation operator
  \end{itemize}
\end{itemize}

If you get stuck, you can check out the hints presented after the cell
below to help you with the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}gradient}
\PY{k}{def} \PY{n+nf}{compute\PYZus{}gradient}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:} 
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the gradient for linear regression }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{      x (ndarray): Shape (m,) Input to the model (Population of cities) }
\PY{l+s+sd}{      y (ndarray): Shape (m,) Label (Actual profits for the cities)}
\PY{l+s+sd}{      w, b (scalar): Parameters of the model  }
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{      dj\PYZus{}dw (scalar): The gradient of the cost w.r.t. the parameters w}
\PY{l+s+sd}{      dj\PYZus{}db (scalar): The gradient of the cost w.r.t. the parameter b     }
\PY{l+s+sd}{     \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} Number of training examples}
    \PY{n}{m} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} You need to return the following variables correctly}
    \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{dj\PYZus{}db} \PY{o}{=} \PY{l+m+mi}{0}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
        \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{n}{dj\PYZus{}dw} \PY{o}{+} \PY{p}{(}\PY{p}{(}\PY{n}{w} \PY{o}{*} \PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+} \PY{n}{b}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]}
        \PY{n}{dj\PYZus{}db} \PY{o}{=} \PY{n}{dj\PYZus{}db} \PY{o}{+} \PY{p}{(}\PY{p}{(}\PY{n}{w} \PY{o}{*} \PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+} \PY{n}{b}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
    \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{n}{dj\PYZus{}dw}\PY{o}{/}\PY{n}{m}
    \PY{n}{dj\PYZus{}db} \PY{o}{=} \PY{n}{dj\PYZus{}db}\PY{o}{/}\PY{n}{m}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} }
        
    \PY{k}{return} \PY{n}{dj\PYZus{}dw}\PY{p}{,} \PY{n}{dj\PYZus{}db}
\end{Verbatim}
\end{tcolorbox}

    Click for hints

\begin{itemize}
\item
  You can represent a summation operator eg:
  \(h = \sum\limits_{i = 0}^{m-1} 2i\) in code as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ h }\OperatorTok{=} \DecValTok{0}
 \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(m):}
\NormalTok{     h }\OperatorTok{=}\NormalTok{ h }\OperatorTok{+} \DecValTok{2}\OperatorTok{*}\NormalTok{i}
\end{Highlighting}
\end{Shaded}

  \begin{itemize}
  \item
    In this case, you can iterate over all the examples in \texttt{x}
    using a for loop and for each example, keep adding the gradient from
    that example to the variables \texttt{dj\_dw} and \texttt{dj\_db}
    which are initialized outside the loop.
  \item
    Then, you can return \texttt{dj\_dw} and \texttt{dj\_db} both
    divided by \texttt{m}.\\

    Click for more hints
  \item
    Here's how you can structure the overall implementation for this
    function
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ compute\_gradient(x, y, w, b): }
    \CommentTok{"""}
\CommentTok{    Computes the gradient for linear regression }
\CommentTok{    Args:}
\CommentTok{      x (ndarray): Shape (m,) Input to the model (Population of cities) }
\CommentTok{      y (ndarray): Shape (m,) Label (Actual profits for the cities)}
\CommentTok{      w, b (scalar): Parameters of the model  }
\CommentTok{    Returns}
\CommentTok{      dj\_dw (scalar): The gradient of the cost w.r.t. the parameters w}
\CommentTok{      dj\_db (scalar): The gradient of the cost w.r.t. the parameter b     }
\CommentTok{    """}

    \CommentTok{\# Number of training examples}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ x.shape[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# You need to return the following variables correctly}
\NormalTok{    dj\_dw }\OperatorTok{=} \DecValTok{0}
\NormalTok{    dj\_db }\OperatorTok{=} \DecValTok{0}

    \CommentTok{\#\#\# START CODE HERE }\AlertTok{\#\#\#}\CommentTok{ }
    \CommentTok{\# Loop over examples}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(m):  }
        \CommentTok{\# Your code here to get prediction f\_wb for the ith example}
\NormalTok{        f\_wb }\OperatorTok{=} 

        \CommentTok{\# Your code here to get the gradient for w from the ith example }
\NormalTok{        dj\_dw\_i }\OperatorTok{=} 

        \CommentTok{\# Your code here to get the gradient for b from the ith example }
\NormalTok{        dj\_db\_i }\OperatorTok{=} 

        \CommentTok{\# Update dj\_db : In Python, a += 1  is the same as a = a + 1}
\NormalTok{        dj\_db }\OperatorTok{+=}\NormalTok{ dj\_db\_i}

        \CommentTok{\# Update dj\_dw}
\NormalTok{        dj\_dw }\OperatorTok{+=}\NormalTok{ dj\_dw\_i}

    \CommentTok{\# Divide both dj\_dw and dj\_db by m}
\NormalTok{    dj\_dw }\OperatorTok{=}\NormalTok{ dj\_dw }\OperatorTok{/}\NormalTok{ m}
\NormalTok{    dj\_db }\OperatorTok{=}\NormalTok{ dj\_db }\OperatorTok{/}\NormalTok{ m}
    \CommentTok{\#\#\# }\RegionMarkerTok{END}\CommentTok{ CODE HERE }\AlertTok{\#\#\#}\CommentTok{ }

    \ControlFlowTok{return}\NormalTok{ dj\_dw, dj\_db}
\end{Highlighting}
\end{Shaded}

  \begin{itemize}
  \tightlist
  \item
    If you're still stuck, you can check the hints presented below to
    figure out how to calculate \texttt{f\_wb} and \texttt{cost}.
  \end{itemize}

  Hint to calculate f\_wb     You did this in the previous exercise! For
  scalars \(a\), \(b\) and \(c\) (x{[}i{]}, w and b are all scalars),
  you can calculate the equation \(h = ab + c\) in code as h = a * b + c

      More hints to calculate f     You can compute f\_wb as f\_wb = w *
  x{[}i{]} + b

  Hint to calculate dj\_dw\_i     For scalars \(a\), \(b\) and \(c\)
  (f\_wb, y{[}i{]} and x{[}i{]} are all scalars), you can calculate the
  equation \(h = (a - b)c\) in code as h = (a-b)*c

      More hints to calculate f     You can compute dj\_dw\_i as
  dj\_dw\_i = (f\_wb - y{[}i{]}) * x{[}i{]}

  Hint to calculate dj\_db\_i     You can compute dj\_db\_i as dj\_db\_i
  = f\_wb - y{[}i{]}
\end{itemize}

    Run the cells below to check your implementation of the
\texttt{compute\_gradient} function with two different initializations
of the parameters \(w\),\(b\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Compute and display gradient with w initialized to zeroes}
\PY{n}{initial\PYZus{}w} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{l+m+mi}{0}

\PY{n}{tmp\PYZus{}dj\PYZus{}dw}\PY{p}{,} \PY{n}{tmp\PYZus{}dj\PYZus{}db} \PY{o}{=} \PY{n}{compute\PYZus{}gradient}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient at initial w, b (zeros):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tmp\PYZus{}dj\PYZus{}dw}\PY{p}{,} \PY{n}{tmp\PYZus{}dj\PYZus{}db}\PY{p}{)}

\PY{n}{compute\PYZus{}gradient\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}gradient}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Gradient at initial w, b (zeros): -65.32884974555672 -5.83913505154639
Using X with shape (4, 1)
\textcolor{ansi-green-intense}{All tests passed!}
    \end{Verbatim}

    Now let's run the gradient descent algorithm implemented above on our
dataset.

\textbf{Expected Output}:

Gradient at initial , b (zeros)

-65.32884975 -5.83913505154639

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Compute and display cost and gradient with non\PYZhy{}zero w}
\PY{n}{test\PYZus{}w} \PY{o}{=} \PY{l+m+mf}{0.2}
\PY{n}{test\PYZus{}b} \PY{o}{=} \PY{l+m+mf}{0.2}
\PY{n}{tmp\PYZus{}dj\PYZus{}dw}\PY{p}{,} \PY{n}{tmp\PYZus{}dj\PYZus{}db} \PY{o}{=} \PY{n}{compute\PYZus{}gradient}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{test\PYZus{}w}\PY{p}{,} \PY{n}{test\PYZus{}b}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient at test w, b:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tmp\PYZus{}dj\PYZus{}dw}\PY{p}{,} \PY{n}{tmp\PYZus{}dj\PYZus{}db}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Gradient at test w, b: -47.41610118114435 -4.007175051546391
    \end{Verbatim}

    \textbf{Expected Output}:

Gradient at test w

-47.41610118 -4.007175051546391

    \#\#\# 2.6 Learning parameters using batch gradient descent

You will now find the optimal parameters of a linear regression model by
using batch gradient descent. Recall batch refers to running all the
examples in one iteration. - You don't need to implement anything for
this part. Simply run the cells below.

\begin{itemize}
\item
  A good way to verify that gradient descent is working correctly is to
  look at the value of \(J(w,b)\) and check that it is decreasing with
  each step.
\item
  Assuming you have implemented the gradient and computed the cost
  correctly and you have an appropriate value for the learning rate
  alpha, \(J(w,b)\) should never increase and should converge to a
  steady value by the end of the algorithm.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{gradient\PYZus{}descent}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w\PYZus{}in}\PY{p}{,} \PY{n}{b\PYZus{}in}\PY{p}{,} \PY{n}{cost\PYZus{}function}\PY{p}{,} \PY{n}{gradient\PYZus{}function}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{num\PYZus{}iters}\PY{p}{)}\PY{p}{:} 
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Performs batch gradient descent to learn theta. Updates theta by taking }
\PY{l+s+sd}{    num\PYZus{}iters gradient steps with learning rate alpha}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{      x :    (ndarray): Shape (m,)}
\PY{l+s+sd}{      y :    (ndarray): Shape (m,)}
\PY{l+s+sd}{      w\PYZus{}in, b\PYZus{}in : (scalar) Initial values of parameters of the model}
\PY{l+s+sd}{      cost\PYZus{}function: function to compute cost}
\PY{l+s+sd}{      gradient\PYZus{}function: function to compute the gradient}
\PY{l+s+sd}{      alpha : (float) Learning rate}
\PY{l+s+sd}{      num\PYZus{}iters : (int) number of iterations to run gradient descent}
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{      w : (ndarray): Shape (1,) Updated values of parameters of the model after}
\PY{l+s+sd}{          running gradient descent}
\PY{l+s+sd}{      b : (scalar)                Updated value of parameter of the model after}
\PY{l+s+sd}{          running gradient descent}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} number of training examples}
    \PY{n}{m} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} An array to store cost J and w\PYZsq{}s at each iteration — primarily for graphing later}
    \PY{n}{J\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{w\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{w} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{w\PYZus{}in}\PY{p}{)}  \PY{c+c1}{\PYZsh{}avoid modifying global w within function}
    \PY{n}{b} \PY{o}{=} \PY{n}{b\PYZus{}in}
    
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}iters}\PY{p}{)}\PY{p}{:}

        \PY{c+c1}{\PYZsh{} Calculate the gradient and update the parameters}
        \PY{n}{dj\PYZus{}dw}\PY{p}{,} \PY{n}{dj\PYZus{}db} \PY{o}{=} \PY{n}{gradient\PYZus{}function}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b} \PY{p}{)}  

        \PY{c+c1}{\PYZsh{} Update Parameters using w, b, alpha and gradient}
        \PY{n}{w} \PY{o}{=} \PY{n}{w} \PY{o}{\PYZhy{}} \PY{n}{alpha} \PY{o}{*} \PY{n}{dj\PYZus{}dw}               
        \PY{n}{b} \PY{o}{=} \PY{n}{b} \PY{o}{\PYZhy{}} \PY{n}{alpha} \PY{o}{*} \PY{n}{dj\PYZus{}db}               

        \PY{c+c1}{\PYZsh{} Save cost J at each iteration}
        \PY{k}{if} \PY{n}{i}\PY{o}{\PYZlt{}}\PY{l+m+mi}{100000}\PY{p}{:}      \PY{c+c1}{\PYZsh{} prevent resource exhaustion }
            \PY{n}{cost} \PY{o}{=}  \PY{n}{cost\PYZus{}function}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}
            \PY{n}{J\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cost}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Print cost every at intervals 10 times or as many iterations if \PYZlt{} 10}
        \PY{k}{if} \PY{n}{i}\PY{o}{\PYZpc{}} \PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n}{num\PYZus{}iters}\PY{o}{/}\PY{l+m+mi}{10}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{w\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{w}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iteration }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{:}\PY{l+s+s2}{4}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: Cost }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{float}\PY{p}{(}\PY{n}{J\PYZus{}history}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{8.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{   }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
    \PY{k}{return} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{J\PYZus{}history}\PY{p}{,} \PY{n}{w\PYZus{}history} \PY{c+c1}{\PYZsh{}return w and J,w history for graphing}
\end{Verbatim}
\end{tcolorbox}

    Now let's run the gradient descent algorithm above to learn the
parameters for our dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} initialize fitting parameters. Recall that the shape of w is (n,)}
\PY{n}{initial\PYZus{}w} \PY{o}{=} \PY{l+m+mf}{0.}
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{l+m+mf}{0.}

\PY{c+c1}{\PYZsh{} some gradient descent settings}
\PY{n}{iterations} \PY{o}{=} \PY{l+m+mi}{1500}
\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n}{w}\PY{p}{,}\PY{n}{b}\PY{p}{,}\PY{n}{\PYZus{}}\PY{p}{,}\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{gradient\PYZus{}descent}\PY{p}{(}\PY{n}{x\PYZus{}train} \PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{,} 
                     \PY{n}{compute\PYZus{}cost}\PY{p}{,} \PY{n}{compute\PYZus{}gradient}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{iterations}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{w,b found by gradient descent:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration    0: Cost     6.74
Iteration  150: Cost     5.31
Iteration  300: Cost     4.96
Iteration  450: Cost     4.76
Iteration  600: Cost     4.64
Iteration  750: Cost     4.57
Iteration  900: Cost     4.53
Iteration 1050: Cost     4.51
Iteration 1200: Cost     4.50
Iteration 1350: Cost     4.49
w,b found by gradient descent: 1.166362350335582 -3.63029143940436
    \end{Verbatim}

    \textbf{Expected Output}:

w, b found by gradient descent

1.16636235 -3.63029143940436

    We will now use the final parameters from gradient descent to plot the
linear fit.

Recall that we can get the prediction for a single example
\(f(x^{(i)})= wx^{(i)}+b\).

To calculate the predictions on the entire dataset, we can loop through
all the training examples and calculate the prediction for each example.
This is shown in the code block below.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{m} \PY{o}{=} \PY{n}{x\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{predicted} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{m}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
    \PY{n}{predicted}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{w} \PY{o}{*} \PY{n}{x\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+} \PY{n}{b}
\end{Verbatim}
\end{tcolorbox}

    We will now plot the predicted values to see the linear fit.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot the linear fit}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{predicted}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create a scatter plot of the data. }
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 

\PY{c+c1}{\PYZsh{} Set the title}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Profits vs. Population per city}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Set the y\PYZhy{}axis label}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Profit in \PYZdl{}10,000}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Set the x\PYZhy{}axis label}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Population of City in 10,000s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 0, 'Population of City in 10,000s')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_43_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Your final values of \(w,b\) can also be used to make predictions on
profits. Let's predict what the profit would be in areas of 35,000 and
70,000 people.

\begin{itemize}
\item
  The model takes in population of a city in 10,000s as input.
\item
  Therefore, 35,000 people can be translated into an input to the model
  as \texttt{np.array({[}3.5{]})}
\item
  Similarly, 70,000 people can be translated into an input to the model
  as \texttt{np.array({[}7.{]})}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{predict1} \PY{o}{=} \PY{l+m+mf}{3.5} \PY{o}{*} \PY{n}{w} \PY{o}{+} \PY{n}{b}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{For population = 35,000, we predict a profit of \PYZdl{}}\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{predict1}\PY{o}{*}\PY{l+m+mi}{10000}\PY{p}{)}\PY{p}{)}

\PY{n}{predict2} \PY{o}{=} \PY{l+m+mf}{7.0} \PY{o}{*} \PY{n}{w} \PY{o}{+} \PY{n}{b}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{For population = 70,000, we predict a profit of \PYZdl{}}\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{predict2}\PY{o}{*}\PY{l+m+mi}{10000}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
For population = 35,000, we predict a profit of \$4519.77
For population = 70,000, we predict a profit of \$45342.45
    \end{Verbatim}

    \textbf{Expected Output}:

For population = 35,000, we predict a profit of

\$4519.77

For population = 70,000, we predict a profit of

\$45342.45

    \textbf{Congratulations on completing this practice lab on linear
regression! Next week, you will create models to solve a different type
of problem: classification. See you there!}

    Please click here if you want to experiment with any of the non-graded
code.

Important Note: Please only do this when you've already passed the
assignment to avoid problems with the autograder.

On the notebook's menu, click ``View'' \textgreater{} ``Cell Toolbar''
\textgreater{} ``Edit Metadata''

Hit the ``Edit Metadata'' button next to the code cell which you want to
lock/unlock

Set the attribute value for ``editable'' to:

``true'' if you want to unlock it

``false'' if you want to lock it

\begin{verbatim}
    </li>
    <li> On the notebook’s menu, click “View” > “Cell Toolbar” > “None” </li>
</ol>
<p> Here's a short demo of how to do the steps above: 
    <br>
    <img src="https://lh3.google.com/u/0/d/14Xy_Mb17CZVgzVAgq7NCjMVBvSae3xO1" align="center" alt="unlock_cells.gif">
\end{verbatim}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
